# HPO_Rev

In many applications and problems, it has been observed that an optimal solution can be achieved by just configuring better existing machine learning techniques rather than inventing new ones, and to get them to work well, hyper-parameter tuning is a crucial step. There is also a big number of machine learning practitioners for whom, with a lot of menial work involved, grid search and manual search are the de facto strategies to use for hyper-parameter optimization, only accelerated by domain knowledge. Automated hyperparameter tuning can reduce the human effort necessary for applying an optimized machine learning algorithm. This paper is thus, an overview of the most prominent methods for hyper-parameter optimization (HPO), knowledge transference (meta-learning) and Neural Architecture Search
